### Experience of Java and Hadoop

#### 1. Two consecutive MapReduce jobs

In consecutive MapReduce jobs, the intermediate results are written back to HDFS. Those intermediate resuts are usually put into a temperate file. 

##### Data type of intermediate results

The data types of two MapReduce jobs do not need to be match. Actually, Hadoop itself will not follow the match rule. Within one MapReduce, the output of the *map* function should share the same data type with the input of *reduce* function. 

However, for HDFS storage, the data type will not be guaranteed. Especially, when the first *reducer* function has a key class *IntWritable*, or *Text* generated by *Integer*.  Here is a [reference from Stackoverflow](https://stackoverflow.com/questions/14922087/hadoop-longwritable-cannot-be-cast-to-org-apache-hadoop-io-intwritable) : 

> The key class of a mapper that maps text files is always LongWritable. That is because it contains the byte offset of the current line and this could easily overflow an integer.

- For the second MapReduce, when the intermediate results contain only numbers, the input *key* class could be set to *LongWritable*, and *value* class is *Text*.

- Regardless what the first *reduce* function has stored in HDFS, the second *map* class could regard the input key as line number (read in as *LongWritable*), and choose to not utilize the key. Then split the *value* (read in as *Text*) and do some transform (like *StringTokenizer* and *Integer.parseInt*).

- When the output *key* class of the first *reduce* function is alphabet made *Text*, *LongWritable* may be avoided. 


##### class and package

New data type may be useful (like *Pair*), but **There should be only one defination of a class within one package**.  If a class has been used by many other MpaReduce programs, set up new packages.


##### variables

Variables should **not** be difined twice within one block. To be safe, avoid any name conflict within one file.

When not using the construction method to create new object, the keyword *new* should be omitted. For example:

		Integer val = Integer.parseInt(...);



##### main function of the MpaReduce class

Big changes are introduced into the *main* function. The major body is simplified as a *ToolRunner.run*:

		public static void main(String[] args) throws Exception {
        	int res = ToolRunner.run(new Configuration(), new TopPopularLinks(), args);
        	System.exit(res);
    	}


The (important) configuration information are in the *run* function:

	@Override
    public int run(String[] args) throws Exception {
		Configuration conf = this.getConf();
		FileSystem fs = FileSystem.get(conf);
		Path tmpPath = new Path("/mp2/tmp");                             // set up temerate file in HDFS, delete existing one
		fs.delete(tmpPath, true);
		
		Job jobA = Job.getInstance(conf, "Link Count");
		jobA.setOutputKeyClass(Text.class);
		jobA.setOutputValueClass(IntWritable.class);

		jobA.setMapperClass(LinkCountMap.class);
		jobA.setReducerClass(LinkCountReduce.class);

		FileInputFormat.setInputPaths(jobA, new Path(args[0]));			// take Command Line Input argument as the data path
		FileOutputFormat.setOutputPath(jobA, tmpPath);
		
		jobA.setJarByClass(TopPopularLinks.class);
		jobA.waitForCompletion(true);									// waitForCompletion() should not appear before other methods.

		Job jobB = Job.getInstance(conf, "Top Links");
		jobB.setOutputKeyClass(IntWritable.class);
		jobB.setOutputValueClass(IntWritable.class);
		
		jobB.setMapOutputKeyClass(NullWritable.class);
		jobB.setMapOutputValueClass(IntArrayWritable.class);
		
		jobB.setMapperClass(TopLinksMap.class);
		jobB.setReducerClass(TopLinksReduce.class);
		jobB.setNumReduceTasks(1);
		
		FileInputFormat.setInputPaths(jobB, tmpPath);
		FileOutputFormat.setOutputPath(jobB, new Path(args[1]));

		//jobB.setInputFormatClass(KeyValueTextInputFormat.class);
        //jobB.setOutputFormatClass(TextOutputFormat.class);

		jobB.setJarByClass(TopPopularLinks.class);
		return jobB.waitForCompletion(true) ? 0 : 1;
    }



###### 1. the setJarByClass

**setJarByClass** is quite import for both jobA and jobB. Without this clain, Hadoop will not be able to find the corresponding Mapper and Reducer classes.


###### 2. waitForCompletion

This method should always be put in the **last** of all corresponding configurations claims. 




#### The setup and cleanup methods

The setup and cleanup methods of Mapper/Reducer in Hadoop MapReduce are called for each task, so if you have 20 mappers (or reducer) running, the setup / cleanup will be called for each one.  The implementation scheme of Hadoop MapReduce is as follows:

		public void run(Context context) throws IOException, InterruptedException {
    		setup(context);
    		try {
      		while (context.nextKey()) {
        		reduce(context.getCurrentKey(), context.getValues(), context);
      		}
    		} finally {
      			cleanup(context);
    		}
  		}


Based on this, it is super userful to Override the setup and cleanup methods. For example, use *setup* to set the attributes/parameters:

		@Override
        protected void setup(Context context) throws IOException,InterruptedException {
            Configuration conf = context.getConfiguration();
            this.N = conf.getInt("N", 10);
        }

Override *map* to store *(kye,value)* into *TreeSet* for sorting:

		@Override
		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
			String line = value.toString();
			StringTokenizer tokenizer = new StringTokenizer(line, " \t,;.?!-:@[](){}_*/");
			String pageID = tokenizer.nextToken().trim().toLowerCase();
			Integer page = Integer.parseInt(pageID);
			String num = tokenizer.nextToken().trim().toLowerCase();
			Integer count = Integer.parseInt(num);
			countToLinkMap.add(new Pair<Integer, Integer>(count, page));
			if (countToLinkMap.size() > this.N) {
				countToLinkMap.remove(countToLinkMap.first());
		} 


Then Override the *cleanup* method to emit *(key, value)* pair:

		@Override
		protected void cleanup(Context context) throws IOException, InterruptedException {
			for (Pair<Integer, Integer> item: countToLinkMap) {
				Integer[] integers = {item.second.intValue(), item.first.intValue()};
				IntArrayWritable val = new IntArrayWritable(integers);
				context.write(NullWritable.get(), val);
			}
		}


Both *Mapper* and *Reducer* classes have the same pattern.



##### Combiner

Combiner is not always applicable. It works only if the reduce function is commutative and associative. The sum function is both commutative and associateive, but average or median function are not.